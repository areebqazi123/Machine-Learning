{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55dff7cd-a6fe-4df2-aa23-0acbf8e12d75",
   "metadata": {},
   "source": [
    "### Model Drift Monitoring\n",
    "\n",
    "Machine learning models are often deployed into dynamic environments where the data distribution may evolve over time. This phenomenon, known as **model drift**, can significantly impact the performance of the deployed model. Monitoring and mitigating model drift is critical to ensure continued reliability and accuracy in predictions.\n",
    "\n",
    "#### Types of Model Drift\n",
    "\n",
    "1. **Data Drift (Covariate Shift):**\n",
    "   Data drift occurs when the input data distribution changes over time. This may happen due to external factors such as changes in user behavior, market trends, or sensor recalibration. For instance, a recommendation system trained on historical user preferences may perform poorly if user preferences change drastically due to seasonal events.\n",
    "\n",
    "   **Example:**\n",
    "   - Training Data Distribution:\n",
    "    $ P_{train}(X) \\sim N(\\mu_{train}, \\sigma_{train}^2) $\n",
    "   - Deployment Data Distribution:\n",
    "    $ P_{deploy}(X) \\sim N(\\mu_{deploy}, \\sigma_{deploy}^2) $, where$ \\mu_{deploy} \\neq \\mu_{train} $.\n",
    "\n",
    "2. **Concept Drift:**\n",
    "   Concept drift refers to changes in the relationship between input features and target labels. This could occur if the underlying process generating the data evolves.\n",
    "\n",
    "   **Example:**\n",
    "   - Original Relationship:$ Y = \\beta_0 + \\beta_1 X + \\epsilon $\n",
    "   - New Relationship:$ Y = \\beta_0' + \\beta_1' X + \\epsilon' $, where$ \\beta_1' \\neq \\beta_1 $.\n",
    "\n",
    "   A fraud detection model might experience concept drift if fraudsters adapt their strategies over time.\n",
    "\n",
    "3. **Label Drift:**\n",
    "   Label drift happens when the distribution of the target variable changes. This is common in scenarios where the frequency of certain events changes over time, such as a decrease in fraudulent transactions due to improved detection systems.\n",
    "\n",
    "   **Example:**\n",
    "   - Training Target Distribution:$ P_{train}(Y) \\sim \\text{Multinomial}(\\theta_{train}) $\n",
    "   - Deployment Target Distribution:$ P_{deploy}(Y) \\sim \\text{Multinomial}(\\theta_{deploy}) $, where$ \\theta_{deploy} \\neq \\theta_{train} $.\n",
    "\n",
    "#### Methods for Monitoring Drift\n",
    "\n",
    "1. **Statistical Testing:**\n",
    "   - **Kolmogorov-Smirnov (KS) Test:** Measures the maximum distance between the empirical distribution functions of two datasets.\n",
    "     $ D_{KS} = \\sup_x |F_{1}(x) - F_{2}(x)| $\n",
    "     where$ F_{1}(x) $ and$ F_{2}(x) $ are the cumulative distribution functions of the training and deployment data, respectively.\n",
    "   - **Chi-Square Test:** Used for categorical data to compare observed and expected distributions.\n",
    "     $ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i} $\n",
    "   - **t-Test:** Compares the means of two datasets to detect shifts in numeric data.\n",
    "     $ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} $\n",
    "\n",
    "2. **Drift Metrics:**\n",
    "   - **Population Stability Index (PSI):** Measures the stability of a variableâ€™s distribution over time. Values above 0.25 indicate significant drift.\n",
    "     $ PSI = \\sum \\left( (P_{i}^{train} - P_{i}^{deploy}) \\ln \\frac{P_{i}^{train}}{P_{i}^{deploy}} \\right) $\n",
    "   - **Jensen-Shannon Divergence:** Quantifies the similarity between two probability distributions.\n",
    "     $ JSD(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M) $\n",
    "     where$ M = \\frac{1}{2}(P + Q) $.\n",
    "\n",
    "3. **Feature Importance Monitoring:**\n",
    "   Track changes in feature importance as determined by the model. Sudden shifts may indicate concept drift.\n",
    "\n",
    "4. **Retraining and Re-Evaluation:**\n",
    "   Regularly retrain the model on updated data and evaluate its performance to detect degradation.\n",
    "\n",
    "#### Example Implementation\n",
    "\n",
    "Consider a scenario where a credit risk prediction model is deployed, and the goal is to monitor for data drift in income levels (a key feature):\n",
    "\n",
    "- **Step 1:** Compute KS statistic between the training and recent deployment data distributions for income levels.\n",
    "- **Step 2:** Set a threshold (e.g.,$ D_{KS} > 0.1 $) for detecting drift.\n",
    "- **Step 3:** If drift is detected, retrain the model on updated data and revalidate its performance metrics.\n",
    "\n",
    "#### Hypothesis Testing for Drift\n",
    "\n",
    "1. **Null Hypothesis ($H_0 $):**\n",
    "   - Training and deployment data come from the same distribution.\n",
    "2. **Alternative Hypothesis ($H_1 $):**\n",
    "   - Training and deployment data come from different distributions.\n",
    "3. **p-Value Interpretation:**\n",
    "   - A small p-value (e.g.,$ p < 0.05 $) indicates strong evidence against$ H_0 $, suggesting drift.\n",
    "\n",
    "By proactively monitoring model drift using the strategies outlined above, organizations can maintain the reliability and effectiveness of their deployed machine learning systems, ensuring they adapt to changing environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86c643-ac5d-4a22-b654-5d8728652d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
